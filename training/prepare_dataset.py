#!/usr/bin/env python3
"""
æ•°æ®é›†å‡†å¤‡å·¥å…·

åŠŸèƒ½ï¼š
1. æ•°æ®é›†æ ¼å¼è½¬æ¢
2. æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
3. æ•°æ®é›†åˆ’åˆ†ï¼ˆè®­ç»ƒé›†/éªŒè¯é›†ï¼‰
4. æ•°æ®æ ¼å¼éªŒè¯
"""

import pandas as pd
import argparse
import os
import sys
from sklearn.model_selection import train_test_split

def validate_dataset(df):
    """éªŒè¯æ•°æ®é›†æ ¼å¼"""
    errors = []
    
    # æ£€æŸ¥å¿…éœ€çš„åˆ—
    if 'text' not in df.columns:
        errors.append("æ•°æ®é›†ç¼ºå°‘ 'text' åˆ—")
    if 'label' not in df.columns:
        errors.append("æ•°æ®é›†ç¼ºå°‘ 'label' åˆ—")
    
    if errors:
        return False, errors
    
    # æ£€æŸ¥æ ‡ç­¾å€¼
    valid_labels = ['ç§¯æ', 'æ¶ˆæ', 'Positive', 'Negative', 'positive', 'negative']
    invalid_labels = df[~df['label'].isin(valid_labels)]['label'].unique()
    if len(invalid_labels) > 0:
        errors.append(f"å‘ç°æ— æ•ˆæ ‡ç­¾: {invalid_labels.tolist()}")
    
    # æ£€æŸ¥ç©ºå€¼
    if df['text'].isna().any():
        errors.append("å‘ç°ç©ºçš„æ–‡æœ¬å†…å®¹")
    if df['label'].isna().any():
        errors.append("å‘ç°ç©ºçš„æ ‡ç­¾")
    
    # æ£€æŸ¥æ•°æ®é‡
    if len(df) < 100:
        errors.append(f"æ•°æ®é‡å¤ªå°‘ï¼ˆ{len(df)} æ¡ï¼‰ï¼Œå»ºè®®è‡³å°‘ 1000 æ¡")
    
    # æ£€æŸ¥æ•°æ®å¹³è¡¡æ€§
    label_counts = df['label'].value_counts()
    if len(label_counts) < 2:
        errors.append("æ•°æ®é›†åªåŒ…å«ä¸€ä¸ªç±»åˆ«çš„æ ‡ç­¾")
    else:
        min_count = label_counts.min()
        max_count = label_counts.max()
        imbalance_ratio = min_count / max_count
        if imbalance_ratio < 0.5:
            errors.append(f"æ•°æ®ä¸å¹³è¡¡ï¼šæœ€å°ç±»åˆ« {min_count} æ¡ï¼Œæœ€å¤§ç±»åˆ« {max_count} æ¡ï¼ˆæ¯”ä¾‹ {imbalance_ratio:.2f}ï¼‰")
    
    return len(errors) == 0, errors

def normalize_labels(df):
    """æ ‡å‡†åŒ–æ ‡ç­¾æ ¼å¼"""
    label_mapping = {
        'Positive': 'ç§¯æ',
        'Negative': 'æ¶ˆæ',
        'positive': 'ç§¯æ',
        'negative': 'æ¶ˆæ',
        'POSITIVE': 'ç§¯æ',
        'NEGATIVE': 'æ¶ˆæ'
    }
    
    df['label'] = df['label'].map(label_mapping).fillna(df['label'])
    return df

def clean_text(text):
    """æ¸…æ´—æ–‡æœ¬"""
    if pd.isna(text):
        return ""
    
    text = str(text).strip()
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    text = ' '.join(text.split())
    return text

def prepare_dataset(input_file, output_file, validation_split=0.2, shuffle=True):
    """å‡†å¤‡æ•°æ®é›†"""
    print(f"ğŸ“– è¯»å–æ•°æ®é›†: {input_file}")
    
    # è¯»å– CSV æ–‡ä»¶
    try:
        df = pd.read_csv(input_file, encoding='utf-8')
    except UnicodeDecodeError:
        print("âš ï¸  UTF-8 ç¼–ç å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç ...")
        df = pd.read_csv(input_file, encoding='gbk')
    
    print(f"   åŸå§‹æ•°æ®é‡: {len(df)} æ¡")
    
    # æ ‡å‡†åŒ–æ ‡ç­¾
    print("ğŸ”¤ æ ‡å‡†åŒ–æ ‡ç­¾æ ¼å¼...")
    df = normalize_labels(df)
    
    # æ¸…æ´—æ–‡æœ¬
    print("ğŸ§¹ æ¸…æ´—æ–‡æœ¬æ•°æ®...")
    df['text'] = df['text'].apply(clean_text)
    
    # ç§»é™¤ç©ºæ–‡æœ¬
    df = df[df['text'].str.len() > 0]
    print(f"   æ¸…æ´—åæ•°æ®é‡: {len(df)} æ¡")
    
    # éªŒè¯æ•°æ®é›†
    print("âœ… éªŒè¯æ•°æ®é›†æ ¼å¼...")
    is_valid, errors = validate_dataset(df)
    
    if not is_valid:
        print("âŒ æ•°æ®é›†éªŒè¯å¤±è´¥ï¼š")
        for error in errors:
            print(f"   - {error}")
        return False
    
    print("   âœ… æ•°æ®é›†æ ¼å¼æ­£ç¡®")
    
    # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
    print("\nğŸ“Š æ•°æ®ç»Ÿè®¡ï¼š")
    print(f"   æ€»æ•°æ®é‡: {len(df)} æ¡")
    label_counts = df['label'].value_counts()
    for label, count in label_counts.items():
        print(f"   {label}: {count} æ¡ ({count/len(df)*100:.1f}%)")
    
    # åˆ’åˆ†æ•°æ®é›†
    if validation_split > 0:
        print(f"\nğŸ“¦ åˆ’åˆ†æ•°æ®é›†ï¼ˆéªŒè¯é›†æ¯”ä¾‹: {validation_split}ï¼‰...")
        train_df, val_df = train_test_split(
            df, 
            test_size=validation_split, 
            stratify=df['label'],
            random_state=42,
            shuffle=shuffle
        )
        
        train_file = output_file.replace('.csv', '_train.csv')
        val_file = output_file.replace('.csv', '_val.csv')
        
        train_df.to_csv(train_file, index=False, encoding='utf-8')
        val_df.to_csv(val_file, index=False, encoding='utf-8')
        
        print(f"   âœ… è®­ç»ƒé›†: {train_file} ({len(train_df)} æ¡)")
        print(f"   âœ… éªŒè¯é›†: {val_file} ({len(val_df)} æ¡)")
    
    # ä¿å­˜å®Œæ•´æ•°æ®é›†
    df.to_csv(output_file, index=False, encoding='utf-8')
    print(f"\nğŸ’¾ ä¿å­˜æ•°æ®é›†: {output_file}")
    
    return True

def main():
    parser = argparse.ArgumentParser(description='å‡†å¤‡ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ•°æ®é›†')
    parser.add_argument(
        '--input',
        type=str,
        required=True,
        help='è¾“å…¥æ•°æ®é›†æ–‡ä»¶è·¯å¾„ï¼ˆCSV æ ¼å¼ï¼‰'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='dataset_prepared.csv',
        help='è¾“å‡ºæ•°æ®é›†æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: dataset_prepared.csvï¼‰'
    )
    parser.add_argument(
        '--validation-split',
        type=float,
        default=0.2,
        help='éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤: 0.2ï¼‰'
    )
    parser.add_argument(
        '--no-shuffle',
        action='store_true',
        help='ä¸æ‰“ä¹±æ•°æ®'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {args.input}")
        sys.exit(1)
    
    print("=" * 60)
    print("æ•°æ®é›†å‡†å¤‡å·¥å…·")
    print("=" * 60)
    print()
    
    success = prepare_dataset(
        args.input,
        args.output,
        validation_split=args.validation_split,
        shuffle=not args.no_shuffle
    )
    
    if success:
        print("\nâœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
        print(f"\nä¸‹ä¸€æ­¥ï¼šè¿è¡Œè®­ç»ƒè„šæœ¬")
        print(f"   python3 train_chinese_sentiment.py --dataset {args.output}")
    else:
        print("\nâŒ æ•°æ®é›†å‡†å¤‡å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        sys.exit(1)

if __name__ == '__main__':
    main()

