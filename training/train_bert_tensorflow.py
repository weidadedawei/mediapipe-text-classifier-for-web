#!/usr/bin/env python3
"""
TensorFlow BERT Chinese Sentiment Analysis Training Script

This script fine-tunes a pre-trained BERT model (e.g., bert-base-chinese) for sentiment classification.
It uses TensorFlow and Hugging Face Transformers.

Features:
- Loads and validates CSV datasets.
- Fine-tunes BERT for binary classification (positive/negative).
- Exports models to TFLite (for mobile/web) and SavedModel (for TF.js).
- Generates training logs and evaluation reports.

Usage:
    python3 train_bert_tensorflow.py --dataset dataset.csv --output models/chinese_bert_model.tflite
"""

import argparse
import os
import sys
import json
from datetime import datetime
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

try:
    import tensorflow as tf
    from transformers import (
        TFBertForSequenceClassification,
        BertTokenizer,
        BertConfig
    )
except ImportError as e:
    print("âŒ Error: Missing required dependencies.")
    print(f"   Details: {str(e)}")
    print("   Please run: pip install tensorflow transformers pandas scikit-learn numpy")
    sys.exit(1)


def load_dataset(dataset_path):
    """åŠ è½½ CSV æ•°æ®é›†"""
    print(f"ğŸ“– åŠ è½½æ•°æ®é›†: {dataset_path}")
    
    df = pd.read_csv(dataset_path, encoding='utf-8')
    
    # æ£€æŸ¥å¿…è¦çš„åˆ—
    if 'text' not in df.columns or 'label' not in df.columns:
        raise ValueError(f"æ•°æ®é›†å¿…é¡»åŒ…å« 'text' å’Œ 'label' åˆ—ã€‚å½“å‰åˆ—: {list(df.columns)}")
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
    if len(df) == 0:
        raise ValueError("æ•°æ®é›†ä¸ºç©º")

    # æ¸…ç†æ•°æ®
    original_len = len(df)
    df = df.dropna(subset=['text', 'label'])
    df = df[df['text'].astype(str).str.strip() != '']
    
    if len(df) == 0:
        raise ValueError("æ¸…ç†åçš„æ•°æ®é›†ä¸ºç©ºï¼ˆæ‰€æœ‰è¡Œéƒ½åŒ…å«ç©ºå€¼æˆ–ç©ºæ–‡æœ¬ï¼‰")
        
    if len(df) < original_len:
        print(f"   âš ï¸  å·²ç§»é™¤ {original_len - len(df)} æ¡æ— æ•ˆæ•°æ®ï¼ˆç©ºå€¼æˆ–ç©ºæ–‡æœ¬ï¼‰")
    
    # è·å–æ ‡ç­¾æ˜ å°„
    unique_labels = sorted(df['label'].unique())
    label_to_id = {label: idx for idx, label in enumerate(unique_labels)}
    id_to_label = {idx: label for label, idx in label_to_id.items()}
    
    print(f"   âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ")
    print(f"   æ•°æ®é‡: {len(df)} æ¡")
    print(f"   æ ‡ç­¾: {unique_labels}")
    print(f"   æ ‡ç­¾åˆ†å¸ƒ:")
    for label, count in df['label'].value_counts().items():
        print(f"     {label}: {count} æ¡")
    
    return df, label_to_id, id_to_label


def prepare_data(df, tokenizer, label_to_id, max_length=128):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    print(f"\nğŸ“¦ å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆæœ€å¤§é•¿åº¦: {max_length}ï¼‰...")
    
    texts = df['text'].tolist()
    labels = [label_to_id[label] for label in df['label'].tolist()]
    
    # ä½¿ç”¨ tokenizer ç¼–ç æ–‡æœ¬
    print("   æ­£åœ¨ç¼–ç æ–‡æœ¬...")
    encodings = tokenizer(
        texts,
        truncation=True,
        padding='max_length',
        max_length=max_length,
        return_tensors='tf'
    )
    
    # è½¬æ¢ä¸º TensorFlow æ•°æ®é›†
    dataset = tf.data.Dataset.from_tensor_slices((
        {
            'input_ids': encodings['input_ids'],
            'attention_mask': encodings['attention_mask']
        },
        tf.constant(labels, dtype=tf.int32)
    ))
    
    print(f"   âœ… æ•°æ®å‡†å¤‡å®Œæˆ")
    return dataset


def build_model(num_labels, model_name='uer/roberta-small-wwm-chinese-cluecorpussmall'):
    """æ„å»º BERT åˆ†ç±»æ¨¡å‹"""
    print(f"\nâš™ï¸  æ„å»ºæ¨¡å‹: {model_name}")
    
    try:
        # åŠ è½½é¢„è®­ç»ƒçš„ BERT æ¨¡å‹
        config = BertConfig.from_pretrained(
            model_name,
            num_labels=num_labels,
            hidden_act="gelu"  # å¼ºåˆ¶ä½¿ç”¨å…¼å®¹çš„ GELU ç‰ˆæœ¬
        )
        model = TFBertForSequenceClassification.from_pretrained(
            model_name,
            config=config
        )
    except Exception as e:
        raise RuntimeError(f"æ— æ³•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ '{model_name}': {str(e)}\nè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ã€‚")
    
    print(f"   âœ… æ¨¡å‹æ„å»ºå®Œæˆ")
    print(f"   å‚æ•°é‡: {model.count_params():,}")
    
    return model


def train_model(
    dataset_path,
    output_path,
    epochs=3,
    batch_size=16,
    learning_rate=2e-5,
    validation_split=0.2,
    max_length=128,
    model_name='bert-base-chinese'
):
    """è®­ç»ƒä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹"""
    
    print("=" * 70)
    print("TensorFlow BERT - ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹è®­ç»ƒ")
    print("=" * 70)
    print()
    
    # æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶
    if not os.path.exists(dataset_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶: {dataset_path}")
        return False
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = os.path.dirname(output_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    try:
        # æ­¥éª¤ 1: åŠ è½½æ•°æ®é›†
        df, label_to_id, id_to_label = load_dataset(dataset_path)
        
        # æ­¥éª¤ 2: åŠ è½½ tokenizer
        print(f"\nğŸ“¥ åŠ è½½ BERT Tokenizer: {model_name}")
        tokenizer = BertTokenizer.from_pretrained(model_name)
        print(f"   âœ… Tokenizer åŠ è½½æˆåŠŸ")
        
        # æ­¥éª¤ 3: åˆ’åˆ†æ•°æ®é›†
        print(f"\nğŸ“¦ åˆ’åˆ†æ•°æ®é›†ï¼ˆéªŒè¯é›†æ¯”ä¾‹: {validation_split}ï¼‰...")
        train_df, val_df = train_test_split(
            df,
            test_size=validation_split,
            random_state=42,
            stratify=df['label']
        )
        print(f"   âœ… è®­ç»ƒé›†: {len(train_df)} æ¡")
        print(f"   âœ… éªŒè¯é›†: {len(val_df)} æ¡")
        
        # æ­¥éª¤ 4: å‡†å¤‡æ•°æ®
        train_dataset = prepare_data(train_df, tokenizer, label_to_id, max_length)
        val_dataset = prepare_data(val_df, tokenizer, label_to_id, max_length)
        
        # æ‰¹å¤„ç†
        train_dataset = train_dataset.shuffle(1000).batch(batch_size)
        val_dataset = val_dataset.batch(batch_size)
        
        # æ­¥éª¤ 5: æ„å»ºæ¨¡å‹
        num_labels = len(label_to_id)
        model = build_model(num_labels, model_name)
        
        # æ­¥éª¤ 6: ç¼–è¯‘æ¨¡å‹
        print(f"\nğŸ”§ ç¼–è¯‘æ¨¡å‹...")
        
        # æ£€æµ‹ Apple Silicon (M1/M2/M3) Macï¼Œä½¿ç”¨ legacy ä¼˜åŒ–å™¨ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
        import platform
        is_apple_silicon = platform.system() == 'Darwin' and platform.machine() == 'arm64'
        
        if is_apple_silicon:
            # Apple Silicon Mac: ä½¿ç”¨ legacy ä¼˜åŒ–å™¨ï¼ˆé€Ÿåº¦æå‡ 10 å€ï¼‰
            optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)
            print(f"   ä¼˜åŒ–å™¨: Adam (legacy, é’ˆå¯¹ Apple Silicon ä¼˜åŒ–)")
        else:
            # å…¶ä»–å¹³å°: ä½¿ç”¨æ ‡å‡†ä¼˜åŒ–å™¨
            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
            print(f"   ä¼˜åŒ–å™¨: Adam (æ ‡å‡†)")
        
        print(f"   å­¦ä¹ ç‡: {learning_rate}")
        print(f"   æŸå¤±å‡½æ•°: SparseCategoricalCrossentropy")
        
        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
        metrics = ['accuracy']
        
        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)
        
        # æ­¥éª¤ 7: è®­ç»ƒæ¨¡å‹
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        print(f"   è®­ç»ƒè½®æ•°: {epochs}")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   ï¼ˆè¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œå–å†³äºæ•°æ®é‡å’Œç¡¬ä»¶ï¼‰")
        print()
        
        # å›è°ƒå‡½æ•°
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=2,
                restore_best_weights=True
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=1,
                min_lr=1e-6
            )
        ]
        
        history = model.fit(
            train_dataset,
            validation_data=val_dataset,
            epochs=epochs,
            callbacks=callbacks,
            verbose=1
        )
        
        print("   âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        
        # æ­¥éª¤ 8: è¯„ä¼°æ¨¡å‹
        print(f"\nğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        val_predictions = model.predict(val_dataset)
        val_pred_labels = np.argmax(val_predictions.logits, axis=1)
        val_true_labels = np.array([label_to_id[label] for label in val_df['label'].tolist()])
        
        accuracy = accuracy_score(val_true_labels, val_pred_labels)
        print(f"   éªŒè¯é›†å‡†ç¡®ç‡: {accuracy:.4f}")
        
        # åˆ†ç±»æŠ¥å‘Š
        print(f"\n   åˆ†ç±»æŠ¥å‘Š:")
        report = classification_report(
            val_true_labels,
            val_pred_labels,
            target_names=[id_to_label[i] for i in range(num_labels)],
            digits=4
        )
        print(report)
        
        # ä¿å­˜è¯„ä¼°æŠ¥å‘Š
        report_path = output_path.replace('.tflite', '_evaluation.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 70 + "\n")
            f.write("æ¨¡å‹è¯„ä¼°æŠ¥å‘Š\n")
            f.write("=" * 70 + "\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"æ•°æ®é›†: {dataset_path}\n")
            f.write(f"è®­ç»ƒé›†å¤§å°: {len(train_df)} æ¡\n")
            f.write(f"éªŒè¯é›†å¤§å°: {len(val_df)} æ¡\n\n")
            f.write(f"éªŒè¯é›†å‡†ç¡®ç‡: {accuracy:.4f}\n\n")
            f.write("åˆ†ç±»æŠ¥å‘Š:\n")
            f.write(report)
        
        print(f"   ğŸ’¾ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # æ­¥éª¤ 9: å¯¼å‡º TFLite æ¨¡å‹
        print(f"\nğŸ’¾ å¯¼å‡º TFLite æ¨¡å‹...")
        print(f"   è¾“å‡ºè·¯å¾„: {output_path}")
        
        tflite_success = False
        tflite_model = None
        
        # åˆ›å»ºåŒ…è£…æ¨¡å‹ç”¨äº TFLite è½¬æ¢
        # æ³¨æ„ï¼šTFLite è½¬æ¢éœ€è¦ Keras æ¨¡å‹ï¼Œä½† Transformers æ¨¡å‹æ˜¯å‡½æ•°å¼ API
        # æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªåŒ…è£…æ¨¡å‹
        class TFLiteModel(tf.keras.Model):
            def __init__(self, bert_model):
                super().__init__()
                self.bert_model = bert_model
            
            # æ³¨æ„ï¼šinput_signature å¿…é¡»ä¸ call æ–¹æ³•çš„å‚æ•°ç­¾åå®Œå…¨åŒ¹é…
            def call(self, input_ids, attention_mask):
                outputs = self.bert_model({'input_ids': input_ids, 'attention_mask': attention_mask})
                return tf.nn.softmax(outputs.logits)
        
        # åˆ›å»ºåŒ…è£…æ¨¡å‹
        tflite_wrapper = TFLiteModel(model)
        
        # å®šä¹‰è¾“å…¥ç­¾åï¼ˆç”¨äº TFLite è½¬æ¢ï¼‰
        input_signature = [
            tf.TensorSpec(shape=[None, max_length], dtype=tf.int32, name='input_ids'),
            tf.TensorSpec(shape=[None, max_length], dtype=tf.int32, name='attention_mask')
        ]
        
        # åˆ›å»ºå¸¦ç­¾åçš„æ¨ç†å‡½æ•°
        @tf.function(input_signature=input_signature)
        def model_inference(input_ids, attention_mask):
            return tflite_wrapper(input_ids, attention_mask)
        
        # æµ‹è¯•æ¨¡å‹
        test_input_ids = tf.zeros((1, max_length), dtype=tf.int32)
        test_attention_mask = tf.ones((1, max_length), dtype=tf.int32)
        _ = model_inference(test_input_ids, test_attention_mask)
        
        # è½¬æ¢ä¸º TFLiteï¼ˆä½¿ç”¨ concrete functionï¼‰
        try:
            concrete_func = model_inference.get_concrete_function()
            converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            tflite_model = converter.convert()
            tflite_success = True
        except Exception as e1:
            # å¦‚æœç¬¬ä¸€ç§æ–¹æ³•å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ SavedModel æ–¹å¼
            print(f"   âš ï¸  æ–¹æ³• 1 å¤±è´¥: {str(e1)}")
            print(f"   å°è¯•æ–¹æ³• 2: ä½¿ç”¨ SavedModel è½¬æ¢...")
            
            try:
                # ä¿å­˜ä¸º SavedModel
                saved_model_path = output_path.replace('.tflite', '_savedmodel')
                # åˆ›å»ºä¸€ä¸ªæ¥å—å­—å…¸è¾“å…¥çš„åŒ…è£…å‡½æ•°
                @tf.function(input_signature=[{
                    'input_ids': tf.TensorSpec(shape=[None, max_length], dtype=tf.int32),
                    'attention_mask': tf.TensorSpec(shape=[None, max_length], dtype=tf.int32)
                }])
                def saved_model_fn(inputs):
                    outputs = model(inputs)
                    return tf.nn.softmax(outputs.logits)
                
                # ä¿å­˜ SavedModel
                tf.saved_model.save(
                    tf.Module(),
                    saved_model_path,
                    signatures={'serving_default': saved_model_fn.get_concrete_function()}
                )
                
                # ä» SavedModel è½¬æ¢ä¸º TFLite
                converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)
                converter.optimizations = [tf.lite.Optimize.DEFAULT]
                tflite_model = converter.convert()
                tflite_success = True
                print(f"   âœ… ä½¿ç”¨æ–¹æ³• 2 æˆåŠŸè½¬æ¢")
            except Exception as e2:
                print(f"   âš ï¸  æ–¹æ³• 2 ä¹Ÿå¤±è´¥: {str(e2)}")
                print(f"   æç¤º: TFLite è½¬æ¢é‡åˆ°é—®é¢˜ï¼Œå°†ä¿å­˜ä¸º SavedModel æ ¼å¼")
                print(f"   æ‚¨å¯ä»¥ä½¿ç”¨ tensorflowjs_converter æ‰‹åŠ¨è½¬æ¢ä¸º TensorFlow.js æ ¼å¼")
                
                # ä¿å­˜ä¸º SavedModel ä½œä¸ºå¤‡é€‰
                saved_model_path = output_path.replace('.tflite', '_savedmodel')
                
                # åˆ›å»ºåŒ…è£…å‡½æ•°ç”¨äº SavedModel
                class SavedModelWrapper(tf.Module):
                    def __init__(self, model):
                        super().__init__()
                        self.model = model
                    
                    @tf.function(input_signature=[{
                        'input_ids': tf.TensorSpec(shape=[None, max_length], dtype=tf.int32),
                        'attention_mask': tf.TensorSpec(shape=[None, max_length], dtype=tf.int32)
                    }])
                    def __call__(self, inputs):
                        outputs = self.model(inputs)
                        return tf.nn.softmax(outputs.logits)
                
                wrapper = SavedModelWrapper(model)
                tf.saved_model.save(wrapper, saved_model_path)
                
                print(f"   âœ… å·²ä¿å­˜ä¸º SavedModel: {saved_model_path}")
                print(f"\n   è½¬æ¢ä¸º TensorFlow.js çš„å‘½ä»¤:")
                print(f"   pip install tensorflowjs")
                print(f"   tensorflowjs_converter \\")
                print(f"       --input_format=tf_saved_model \\")
                print(f"       --output_format=tfjs_graph_model \\")
                print(f"       {saved_model_path} \\")
                print(f"       {saved_model_path}_js/")
                print(f"\n   æˆ–è€…ç›´æ¥ä½¿ç”¨ SavedModel è¿›è¡Œæ¨ç†ï¼ˆPython ç¯å¢ƒï¼‰")
        
        # å¦‚æœ TFLite è½¬æ¢æˆåŠŸï¼Œä¿å­˜æ–‡ä»¶
        if tflite_success and tflite_model:
            with open(output_path, 'wb') as f:
                f.write(tflite_model)
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(output_path) / (1024 * 1024)
            print(f"   âœ… TFLite æ¨¡å‹å¯¼å‡ºæˆåŠŸï¼")
            print(f"   æ¨¡å‹å¤§å°: {file_size:.2f} MB")
            
            if file_size > 100:
                print(f"   âš ï¸  è­¦å‘Š: æ¨¡å‹æ–‡ä»¶è¾ƒå¤§ï¼ˆ{file_size:.2f} MBï¼‰ï¼Œå»ºè®®ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬")
        else:
            print(f"   âš ï¸  TFLite æ¨¡å‹æœªå¯¼å‡ºï¼Œä½†å·²ä¿å­˜ SavedModel æ ¼å¼")
        
        # æ— è®º TFLite æ˜¯å¦æˆåŠŸï¼Œéƒ½ä¿å­˜ SavedModelï¼ˆç”¨äºè½¬æ¢ä¸º TensorFlow.jsï¼‰
        print(f"\nğŸ’¾ ä¿å­˜ SavedModelï¼ˆç”¨äº TensorFlow.js è½¬æ¢ï¼‰...")
        saved_model_path = output_path.replace('.tflite', '_savedmodel')
        
        # åˆ›å»ºåŒ…è£…å‡½æ•°ç”¨äº SavedModel
        class SavedModelWrapper(tf.Module):
            def __init__(self, model):
                super().__init__()
                self.model = model
            
            @tf.function(input_signature=[{
                'input_ids': tf.TensorSpec(shape=[None, max_length], dtype=tf.int32),
                'attention_mask': tf.TensorSpec(shape=[None, max_length], dtype=tf.int32)
            }])
            def __call__(self, inputs):
                outputs = self.model(inputs)
                return tf.nn.softmax(outputs.logits)
        
        wrapper = SavedModelWrapper(model)
        tf.saved_model.save(wrapper, saved_model_path)
        print(f"   âœ… SavedModel å·²ä¿å­˜: {saved_model_path}")
        print(f"\n   è½¬æ¢ä¸º TensorFlow.js çš„å‘½ä»¤:")
        print(f"   pip install tensorflowjs")
        print(f"   tensorflowjs_converter \\")
        print(f"       --input_format=tf_saved_model \\")
        print(f"       --output_format=tfjs_graph_model \\")
        print(f"       {saved_model_path} \\")
        print(f"       {saved_model_path.replace('_savedmodel', '_js')}/")
        
        # æ­¥éª¤ 10: ä¿å­˜è¯æ±‡è¡¨å’Œæ ‡ç­¾æ–‡ä»¶
        print(f"\nğŸ’¾ ä¿å­˜è¾…åŠ©æ–‡ä»¶...")
        
        # ä¿å­˜è¯æ±‡è¡¨
        vocab_path = output_path.replace('.tflite', '_vocab.txt')
        tokenizer.save_vocabulary(os.path.dirname(vocab_path))
        vocab_file = os.path.join(os.path.dirname(vocab_path), 'vocab.txt')
        if os.path.exists(vocab_file):
            os.rename(vocab_file, vocab_path)
        print(f"   âœ… è¯æ±‡è¡¨å·²ä¿å­˜: {vocab_path}")
        
        # ä¿å­˜æ ‡ç­¾æ–‡ä»¶
        labels_path = output_path.replace('.tflite', '_labels.txt')
        with open(labels_path, 'w', encoding='utf-8') as f:
            for i in range(num_labels):
                f.write(f"{id_to_label[i]}\n")
        print(f"   âœ… æ ‡ç­¾æ–‡ä»¶å·²ä¿å­˜: {labels_path}")
        
        # ä¿å­˜æ ‡ç­¾æ˜ å°„
        label_map_path = output_path.replace('.tflite', '_label_map.json')
        with open(label_map_path, 'w', encoding='utf-8') as f:
            json.dump({
                'label_to_id': label_to_id,
                'id_to_label': id_to_label
            }, f, ensure_ascii=False, indent=2)
        print(f"   âœ… æ ‡ç­¾æ˜ å°„å·²ä¿å­˜: {label_map_path}")
        
        # ä¿å­˜è®­ç»ƒæ—¥å¿—
        log_path = output_path.replace('.tflite', '_training_log.txt')
        with open(log_path, 'w', encoding='utf-8') as f:
            f.write("=" * 70 + "\n")
            f.write("è®­ç»ƒæ—¥å¿—\n")
            f.write("=" * 70 + "\n")
            f.write(f"è®­ç»ƒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"æ•°æ®é›†: {dataset_path}\n")
            f.write(f"æ¨¡å‹: {model_name}\n")
            f.write(f"è®­ç»ƒå‚æ•°:\n")
            f.write(f"  è®­ç»ƒè½®æ•°: {epochs}\n")
            f.write(f"  æ‰¹æ¬¡å¤§å°: {batch_size}\n")
            f.write(f"  å­¦ä¹ ç‡: {learning_rate}\n")
            f.write(f"  æœ€å¤§é•¿åº¦: {max_length}\n")
            f.write(f"  éªŒè¯é›†æ¯”ä¾‹: {validation_split}\n\n")
            f.write(f"æ•°æ®ç»Ÿè®¡:\n")
            f.write(f"  è®­ç»ƒé›†: {len(train_df)} æ¡\n")
            f.write(f"  éªŒè¯é›†: {len(val_df)} æ¡\n\n")
            f.write(f"è¯„ä¼°ç»“æœ:\n")
            f.write(f"  éªŒè¯é›†å‡†ç¡®ç‡: {accuracy:.4f}\n\n")
            f.write("åˆ†ç±»æŠ¥å‘Š:\n")
            f.write(report)
        
        print(f"   ğŸ’¾ è®­ç»ƒæ—¥å¿—å·²ä¿å­˜: {log_path}")
        
        print("\n" + "=" * 70)
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        print("=" * 70)
        print(f"\nè¾“å‡ºæ–‡ä»¶:")
        print(f"  - æ¨¡å‹æ–‡ä»¶: {output_path}")
        print(f"  - è¯æ±‡è¡¨: {vocab_path}")
        print(f"  - æ ‡ç­¾æ–‡ä»¶: {labels_path}")
        print(f"  - æ ‡ç­¾æ˜ å°„: {label_map_path}")
        print(f"  - è¯„ä¼°æŠ¥å‘Š: {report_path}")
        print(f"  - è®­ç»ƒæ—¥å¿—: {log_path}")
        print(f"\nä¸‹ä¸€æ­¥:")
        print(f"  1. å°†æ¨¡å‹æ–‡ä»¶éƒ¨ç½²åˆ° Web åº”ç”¨")
        print(f"  2. æ›´æ–° src/config.ts ä¸­çš„æ¨¡å‹è·¯å¾„")
        print(f"  3. ä½¿ç”¨ TensorFlow.js åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯:")
        print(f"   {str(e)}")
        print(f"\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print(f"  1. æ£€æŸ¥æ•°æ®é›†æ ¼å¼æ˜¯å¦æ­£ç¡®")
        print(f"  2. ç¡®ä¿æ•°æ®é‡è¶³å¤Ÿï¼ˆå»ºè®®è‡³å°‘ 1000 æ¡ï¼‰")
        print(f"  3. æ£€æŸ¥å†…å­˜æ˜¯å¦å……è¶³ï¼ˆBERT æ¨¡å‹éœ€è¦è¾ƒå¤§å†…å­˜ï¼‰")
        print(f"  4. å°è¯•å‡å° batch_size æˆ– max_length")
        import traceback
        traceback.print_exc()
        return False


def main():
    parser = argparse.ArgumentParser(
        description='ä½¿ç”¨ TensorFlow å’Œ Transformers è®­ç»ƒä¸­æ–‡æƒ…æ„Ÿåˆ†æ BERT æ¨¡å‹',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬è®­ç»ƒ
  python3 train_bert_tensorflow.py --dataset dataset.csv

  # è‡ªå®šä¹‰å‚æ•°
  python3 train_bert_tensorflow.py \\
      --dataset dataset.csv \\
      --output models/my_model.tflite \\
      --epochs 5 \\
      --batch-size 8 \\
      --max-length 256
        """
    )
    
    parser.add_argument(
        '--dataset',
        type=str,
        required=True,
        help='æ•°æ®é›† CSV æ–‡ä»¶è·¯å¾„ï¼ˆå¿…é¡»åŒ…å« text å’Œ label åˆ—ï¼‰'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='models/chinese_bert_model.tflite',
        help='è¾“å‡ºæ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: models/chinese_bert_model.tfliteï¼‰'
    )
    parser.add_argument(
        '--epochs',
        type=int,
        default=3,
        help='è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤: 3ï¼‰'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=16,
        dest='batch_size',
        help='æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤: 16ï¼Œæ ¹æ®å†…å­˜è°ƒæ•´ï¼‰'
    )
    parser.add_argument(
        '--learning-rate',
        type=float,
        default=2e-5,
        dest='learning_rate',
        help='å­¦ä¹ ç‡ï¼ˆé»˜è®¤: 2e-5ï¼‰'
    )
    parser.add_argument(
        '--validation-split',
        type=float,
        default=0.2,
        dest='validation_split',
        help='éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤: 0.2ï¼‰'
    )
    parser.add_argument(
        '--max-length',
        type=int,
        default=128,
        dest='max_length',
        help='æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤: 128ï¼Œå¯è®¾ç½®ä¸º 256ï¼‰'
    )
    parser.add_argument(
        '--model-name',
        type=str,
        default='bert-base-chinese',
        dest='model_name',
        help='é¢„è®­ç»ƒæ¨¡å‹åç§°ï¼ˆé»˜è®¤: bert-base-chineseï¼‰'
    )
    
    args = parser.parse_args()
    
    success = train_model(
        dataset_path=args.dataset,
        output_path=args.output,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        validation_split=args.validation_split,
        max_length=args.max_length,
        model_name=args.model_name
    )
    
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()

